{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'textattack'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/home/hamdi/Downloads/test/Targeted-Adversarial-Attacks-on-Sentiment-Analysis-Models/demo/demo.ipynb Cell 1\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/hamdi/Downloads/test/Targeted-Adversarial-Attacks-on-Sentiment-Analysis-Models/demo/demo.ipynb#W0sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mtextattack\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/hamdi/Downloads/test/Targeted-Adversarial-Attacks-on-Sentiment-Analysis-Models/demo/demo.ipynb#W0sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mtextattack\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mimport\u001b[39;00m AttackArgs, Attacker\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/hamdi/Downloads/test/Targeted-Adversarial-Attacks-on-Sentiment-Analysis-Models/demo/demo.ipynb#W0sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfrom\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mtextattack\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mwrappers\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mimport\u001b[39;00m HuggingFaceModelWrapper\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'textattack'"
     ]
    }
   ],
   "source": [
    "import textattack\n",
    "from textattack import AttackArgs, Attacker\n",
    "from textattack.models.wrappers import HuggingFaceModelWrapper\n",
    "from textattack.datasets import Dataset\n",
    "from textattack.transformations import (\n",
    "    WordSwapRandomCharacterDeletion, CompositeTransformation,\n",
    "    WordSwapEmbedding, WordSwapWordNet, WordSwapMaskedLM\n",
    ")\n",
    "from textattack.constraints.pre_transformation import RepeatModification, StopwordModification\n",
    "from textattack.search_methods import ParticleSwarmOptimization\n",
    "from textattack.goal_functions import GoalFunction, UntargetedClassification\n",
    "from textattack.goal_function_results import ClassificationGoalFunctionResult\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "import numpy as np\n",
    "import Levenshtein\n",
    "import torch\n",
    "from transformers import BartForConditionalGeneration, BartTokenizer\n",
    "from transformers import DistilBertModel, DistilBertTokenizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import time\n",
    "# Define device (GPU if available, otherwise CPU)\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = \"cpu\"\n",
    "\n",
    "# Load sentiment classification model and tokenizer\n",
    "model_name = \"lxyuan/distilbert-base-multilingual-cased-sentiments-student\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "wrapped_model = HuggingFaceModelWrapper(model, tokenizer)\n",
    "\n",
    "\n",
    "# Define attack parameters\n",
    "target_sentence = \"My grandmother's secret sauce is the best ever made!\"\n",
    "n_decimal = 3\n",
    "min_levenshtein = 30\n",
    "min_length = 40\n",
    "max_length = 60\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom goal function to match target probabilities\n",
    "class RoundedScoreGoal_prob(GoalFunction):\n",
    "    def __init__(self, model, target_scores, label_order=('positive', 'neutral', 'negative'), n_decimal=4):\n",
    "        super().__init__(model)\n",
    "        self.target_scores = np.round(np.array(target_scores), decimals=n_decimal)\n",
    "        self.label_order = label_order\n",
    "        self.n_decimal = n_decimal\n",
    "        self._validate_target_scores()\n",
    "\n",
    "    def _validate_target_scores(self):\n",
    "        \"\"\"Ensure rounded targets sum to ~1 and are valid probabilities.\"\"\"\n",
    "        if not np.isclose(self.target_scores.sum(), 1.0, atol=1e-2):\n",
    "            raise ValueError(\"Rounded target scores must sum to ~1\")\n",
    "        if (self.target_scores < 0).any() or (self.target_scores > 1).any():\n",
    "            raise ValueError(\"All target scores must be between 0 and 1\")\n",
    "\n",
    "    def _is_goal_complete(self, model_output, attacked_text):\n",
    "        \"\"\"Check if the generated text meets the target probabilities.\"\"\"\n",
    "        scores = model_output.numpy().flatten()\n",
    "        rounded_scores_final = np.round(scores, decimals=self.n_decimal)\n",
    "        return np.allclose(rounded_scores_final, self.target_scores)\n",
    "\n",
    "    def _get_score(self, model_output, attacked_text):\n",
    "        \"\"\"Compute score as the negative distance from the target probabilities.\"\"\"\n",
    "        scores = model_output.numpy().flatten()\n",
    "        rounded_scores = np.round(scores, decimals=self.n_decimal)\n",
    "        return -np.linalg.norm(rounded_scores - self.target_scores)\n",
    "\n",
    "    def _process_model_outputs(self, inputs, model_outputs):\n",
    "        probabilities = torch.nn.functional.softmax(model_outputs, dim=-1)\n",
    "        return probabilities\n",
    "\n",
    "    def _goal_function_result_type(self):\n",
    "        return ClassificationGoalFunctionResult\n",
    "\n",
    "\n",
    "# Function to compute sentiment scores with rounding\n",
    "def get_rounded_scores(sentence, model, n_decimal=6):\n",
    "    \"\"\"Compute sentiment scores and round to n_decimal places.\"\"\"\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    scores = torch.softmax(logits, dim=1).numpy()[0]\n",
    "    return np.round(scores, decimals=n_decimal)\n",
    "\n",
    "# Custom constraint for Levenshtein distance and text length\n",
    "class LevenshteinConstraint(textattack.constraints.Constraint):\n",
    "    \"\"\"Ensure the transformed text meets Levenshtein distance and length constraints.\"\"\"\n",
    "    def __init__(self, original_sentence, min_distance, min_length, max_length):\n",
    "        super().__init__(compare_against_original=True)\n",
    "        self.original = original_sentence\n",
    "        self.min_distance = min_distance\n",
    "        self.min_length = min_length\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def _check_constraint(self, transformed_text, reference_text):\n",
    "        candidate = transformed_text.text\n",
    "        if not (self.min_length <= len(candidate) <= self.max_length):\n",
    "            return False\n",
    "        return Levenshtein.distance(self.original, candidate) >= self.min_distance\n",
    "    \n",
    "    # Function to automatically compute target scores and label\n",
    "def compute_target_scores_and_label(sentence, model):\n",
    "    \"\"\"Compute the original sentiment scores and extract the target label.\"\"\"\n",
    "    scores = get_rounded_scores(sentence, model, n_decimal=3)\n",
    "    label = int(np.argmax(scores))  # Extract label with the highest probability and cast to int\n",
    "    return scores, label\n",
    "\n",
    "\n",
    "\n",
    "def generate_best_paraphrase(input_sentence: str,\n",
    "                             min_levenshtein_distance: int = 30,\n",
    "                             min_length: int = 30,\n",
    "                             max_length: int = 60,\n",
    "                             num_return_sequences: int = 20,\n",
    "                             temperature: float = 1.5,\n",
    "                             top_k: int = 50,\n",
    "                             top_p: float = 0.95,\n",
    "                             cosine_similarity_threshold: float = 0.8,\n",
    "                             threshold_attempts = 300) -> str:\n",
    "    \"\"\"\n",
    "    Generates diverse paraphrases for a given input sentence and returns the best one\n",
    "    based on cosine similarity and constraints. Keeps generating paraphrases\n",
    "    until one has a cosine similarity score greater than the threshold.\n",
    "\n",
    "    Args:\n",
    "        input_sentence (str): Sentence to paraphrase.\n",
    "        min_levenshtein_distance (int): Minimum Levenshtein distance from input_sentence.\n",
    "        min_length (int): Minimum length constraint for generated paraphrases.\n",
    "        max_length (int): Maximum length constraint for generated paraphrases.\n",
    "        num_return_sequences (int): Number of paraphrases to generate per attempt.\n",
    "        temperature (float): Temperature parameter for diversity during sampling.\n",
    "        top_k (int): Top-k sampling parameter.\n",
    "        top_p (float): Nucleus sampling (top-p) parameter.\n",
    "        cosine_similarity_threshold (float): Minimum cosine similarity score for valid paraphrase.\n",
    "\n",
    "    Returns:\n",
    "        str: Best paraphrase based on cosine similarity or message indicating failure.\n",
    "    \"\"\"\n",
    "    # Load BART paraphrase model and tokenizer\n",
    "    bart_model = BartForConditionalGeneration.from_pretrained('eugenesiow/bart-paraphrase')\n",
    "    bart_tokenizer = BartTokenizer.from_pretrained('eugenesiow/bart-paraphrase')\n",
    "    bart_model = bart_model.to(device)\n",
    "\n",
    "    # Load DistilBERT model for embeddings\n",
    "    embedding_model = DistilBertModel.from_pretrained('lxyuan/distilbert-base-multilingual-cased-sentiments-student')\n",
    "    embedding_tokenizer = DistilBertTokenizer.from_pretrained('lxyuan/distilbert-base-multilingual-cased-sentiments-student')\n",
    "    embedding_model = embedding_model.to(device)\n",
    "\n",
    "    # Tokenize the input sentence\n",
    "    batch = bart_tokenizer(input_sentence, return_tensors='pt').to(device)\n",
    "\n",
    "\n",
    "    attempts = 0\n",
    "    # Loop to keep generating paraphrases until similarity score > 0.8\n",
    "    while attempts < threshold_attempts:\n",
    "        # Generate diverse paraphrases\n",
    "        generated_ids = bart_model.generate(\n",
    "            batch['input_ids'],\n",
    "            num_return_sequences=num_return_sequences,\n",
    "            num_beams=1,\n",
    "            temperature=temperature,\n",
    "            top_k=top_k,\n",
    "            top_p=top_p,\n",
    "            do_sample=True,\n",
    "            repetition_penalty=1.5,\n",
    "        )\n",
    "        paraphrases = bart_tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "\n",
    "        # Filter based on Levenshtein distance and length\n",
    "        valid_paraphrases = [\n",
    "            p for p in paraphrases if\n",
    "            Levenshtein.distance(input_sentence, p) >= min_levenshtein_distance and\n",
    "            min_length <= len(p) <= max_length\n",
    "        ]\n",
    "\n",
    "        # If valid paraphrases are found, proceed to similarity check\n",
    "        if valid_paraphrases:\n",
    "            # Function to compute embeddings\n",
    "            def compute_embedding(sentence):\n",
    "                inputs = embedding_tokenizer(sentence, return_tensors='pt', truncation=True, padding=True).to(device)\n",
    "                with torch.no_grad():\n",
    "                    outputs = embedding_model(**inputs)\n",
    "                return outputs.last_hidden_state.mean(dim=1).squeeze().cpu().numpy()\n",
    "\n",
    "            # Compute embeddings for the original sentence and valid paraphrases\n",
    "            original_embedding = compute_embedding(input_sentence)\n",
    "            paraphrase_embeddings = [compute_embedding(p) for p in valid_paraphrases]\n",
    "\n",
    "            # Compute cosine similarity\n",
    "            similarities = [cosine_similarity([original_embedding], [embedding])[0][0] for embedding in paraphrase_embeddings]\n",
    "\n",
    "            # Check if any paraphrase meets the similarity threshold\n",
    "            for i, similarity in enumerate(similarities):\n",
    "                if similarity > cosine_similarity_threshold:\n",
    "                    best_paraphrase = valid_paraphrases[i]\n",
    "                    return best_paraphrase\n",
    "\n",
    "        # If no valid paraphrase is found, continue the loop until success\n",
    "\n",
    "    return \"No valid paraphrase found after several attempts.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'CompositeTransformation' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/hamdi/Downloads/test/Targeted-Adversarial-Attacks-on-Sentiment-Analysis-Models/demo/demo.ipynb Cell 3\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/hamdi/Downloads/test/Targeted-Adversarial-Attacks-on-Sentiment-Analysis-Models/demo/demo.ipynb#W2sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Define text transformations\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/hamdi/Downloads/test/Targeted-Adversarial-Attacks-on-Sentiment-Analysis-Models/demo/demo.ipynb#W2sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m transformation \u001b[39m=\u001b[39m CompositeTransformation([\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/hamdi/Downloads/test/Targeted-Adversarial-Attacks-on-Sentiment-Analysis-Models/demo/demo.ipynb#W2sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     WordSwapEmbedding(max_candidates\u001b[39m=\u001b[39m\u001b[39m30\u001b[39m),\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/hamdi/Downloads/test/Targeted-Adversarial-Attacks-on-Sentiment-Analysis-Models/demo/demo.ipynb#W2sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     WordSwapMaskedLM(method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbae\u001b[39m\u001b[39m\"\u001b[39m, max_candidates\u001b[39m=\u001b[39m\u001b[39m40\u001b[39m),\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/hamdi/Downloads/test/Targeted-Adversarial-Attacks-on-Sentiment-Analysis-Models/demo/demo.ipynb#W2sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     WordSwapRandomCharacterDeletion()\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/hamdi/Downloads/test/Targeted-Adversarial-Attacks-on-Sentiment-Analysis-Models/demo/demo.ipynb#W2sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m ])\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/hamdi/Downloads/test/Targeted-Adversarial-Attacks-on-Sentiment-Analysis-Models/demo/demo.ipynb#W2sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m# Define constraints\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/hamdi/Downloads/test/Targeted-Adversarial-Attacks-on-Sentiment-Analysis-Models/demo/demo.ipynb#W2sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m constraints \u001b[39m=\u001b[39m [\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/hamdi/Downloads/test/Targeted-Adversarial-Attacks-on-Sentiment-Analysis-Models/demo/demo.ipynb#W2sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     RepeatModification(),\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/hamdi/Downloads/test/Targeted-Adversarial-Attacks-on-Sentiment-Analysis-Models/demo/demo.ipynb#W2sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     StopwordModification(),\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/hamdi/Downloads/test/Targeted-Adversarial-Attacks-on-Sentiment-Analysis-Models/demo/demo.ipynb#W2sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     LevenshteinConstraint(target_sentence, min_levenshtein, min_length, max_length),\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/hamdi/Downloads/test/Targeted-Adversarial-Attacks-on-Sentiment-Analysis-Models/demo/demo.ipynb#W2sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m ]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'CompositeTransformation' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define text transformations\n",
    "transformation = CompositeTransformation([\n",
    "    WordSwapEmbedding(max_candidates=30),\n",
    "    WordSwapMaskedLM(method=\"bae\", max_candidates=40),\n",
    "    WordSwapRandomCharacterDeletion()\n",
    "])\n",
    "\n",
    "# Define constraints\n",
    "constraints = [\n",
    "    RepeatModification(),\n",
    "    StopwordModification(),\n",
    "    LevenshteinConstraint(target_sentence, min_levenshtein, min_length, max_length),\n",
    "]\n",
    "\n",
    "# Define search method\n",
    "search_method = ParticleSwarmOptimization(pop_size=80, max_iters=40, post_turn_check=True, max_turn_retries=10)\n",
    "\n",
    "\n",
    "# Compute target scores and label dynamically\n",
    "target_scores, label = compute_target_scores_and_label(target_sentence)\n",
    "\n",
    "\n",
    "\n",
    "# Instantiate goal function\n",
    "goal_function = RoundedScoreGoal_prob(\n",
    "    model=wrapped_model,\n",
    "    target_scores=target_scores,\n",
    "    label_order=('positive', 'neutral', 'negative'),\n",
    "    n_decimal=3\n",
    ")\n",
    "\n",
    "\n",
    "# Define and run attack\n",
    "attack = textattack.Attack(goal_function, constraints, transformation, search_method)\n",
    "\n",
    "\n",
    "# Generate paraphrase dynamically\n",
    "generated_sentence = generate_best_paraphrase(target_sentence)\n",
    "start_time = time.time()\n",
    "attack_result = attack.attack(generated_sentence, label)\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f\"Attack took {elapsed_time:.2f} seconds.\")\n",
    "print(f\"The new text satisfying the criteria is: {attack_result}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Giskard",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
