{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textattack\n",
    "from textattack import AttackArgs, Attacker\n",
    "from textattack.models.wrappers import HuggingFaceModelWrapper\n",
    "from textattack.datasets import Dataset\n",
    "from textattack.transformations import (\n",
    "    WordSwapRandomCharacterDeletion, CompositeTransformation,\n",
    "    WordSwapEmbedding, WordSwapWordNet, WordSwapMaskedLM\n",
    ")\n",
    "from textattack.constraints.pre_transformation import RepeatModification, StopwordModification\n",
    "from textattack.search_methods import ParticleSwarmOptimization\n",
    "from textattack.goal_functions import GoalFunction, UntargetedClassification\n",
    "from textattack.goal_function_results import ClassificationGoalFunctionResult\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "import numpy as np\n",
    "import Levenshtein\n",
    "import torch\n",
    "from transformers import BartForConditionalGeneration, BartTokenizer\n",
    "from transformers import DistilBertModel, DistilBertTokenizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Define device (GPU if available, otherwise CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load sentiment classification model and tokenizer\n",
    "model_name = \"lxyuan/distilbert-base-multilingual-cased-sentiments-student\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "wrapped_model = HuggingFaceModelWrapper(model, tokenizer)\n",
    "\n",
    "\n",
    "# Define attack parameters\n",
    "target_sentence = \"My grandmother's secret sauce is the best ever made!\"\n",
    "n_decimal = 3\n",
    "min_levenshtein = 30\n",
    "min_length = 40\n",
    "max_length = 60\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom goal function to match target probabilities\n",
    "class RoundedScoreGoal_prob(GoalFunction):\n",
    "    def __init__(self, model, target_scores, label_order=('positive', 'neutral', 'negative'), n_decimal=4):\n",
    "        super().__init__(model)\n",
    "        self.target_scores = np.round(np.array(target_scores), decimals=n_decimal)\n",
    "        self.label_order = label_order\n",
    "        self.n_decimal = n_decimal\n",
    "        self._validate_target_scores()\n",
    "\n",
    "    def _validate_target_scores(self):\n",
    "        \"\"\"Ensure rounded targets sum to ~1 and are valid probabilities.\"\"\"\n",
    "        if not np.isclose(self.target_scores.sum(), 1.0, atol=1e-2):\n",
    "            raise ValueError(\"Rounded target scores must sum to ~1\")\n",
    "        if (self.target_scores < 0).any() or (self.target_scores > 1).any():\n",
    "            raise ValueError(\"All target scores must be between 0 and 1\")\n",
    "\n",
    "    def _is_goal_complete(self, model_output, attacked_text):\n",
    "        \"\"\"Check if the generated text meets the target probabilities.\"\"\"\n",
    "        scores = model_output.numpy().flatten()\n",
    "        rounded_scores_final = np.round(scores, decimals=self.n_decimal)\n",
    "        return np.allclose(rounded_scores_final, self.target_scores)\n",
    "\n",
    "    def _get_score(self, model_output, attacked_text):\n",
    "        \"\"\"Compute score as the negative distance from the target probabilities.\"\"\"\n",
    "        scores = model_output.numpy().flatten()\n",
    "        rounded_scores = np.round(scores, decimals=self.n_decimal)\n",
    "        return -np.linalg.norm(rounded_scores - self.target_scores)\n",
    "\n",
    "    def _process_model_outputs(self, inputs, model_outputs):\n",
    "        probabilities = torch.nn.functional.softmax(model_outputs, dim=-1)\n",
    "        return probabilities\n",
    "\n",
    "    def _goal_function_result_type(self):\n",
    "        return ClassificationGoalFunctionResult\n",
    "\n",
    "\n",
    "# Function to compute sentiment scores with rounding\n",
    "def get_rounded_scores(sentence, model, n_decimal=6):\n",
    "    \"\"\"Compute sentiment scores and round to n_decimal places.\"\"\"\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    scores = torch.softmax(logits, dim=1).numpy()[0]\n",
    "    return np.round(scores, decimals=n_decimal)\n",
    "\n",
    "# Custom constraint for Levenshtein distance and text length\n",
    "class LevenshteinConstraint(textattack.constraints.Constraint):\n",
    "    \"\"\"Ensure the transformed text meets Levenshtein distance and length constraints.\"\"\"\n",
    "    def __init__(self, original_sentence, min_distance, min_length, max_length):\n",
    "        super().__init__(compare_against_original=True)\n",
    "        self.original = original_sentence\n",
    "        self.min_distance = min_distance\n",
    "        self.min_length = min_length\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def _check_constraint(self, transformed_text, reference_text):\n",
    "        candidate = transformed_text.text\n",
    "        if not (self.min_length <= len(candidate) <= self.max_length):\n",
    "            return False\n",
    "        return Levenshtein.distance(self.original, candidate) >= self.min_distance\n",
    "    \n",
    "    # Function to automatically compute target scores and label\n",
    "def compute_target_scores_and_label(sentence, model):\n",
    "    \"\"\"Compute the original sentiment scores and extract the target label.\"\"\"\n",
    "    scores = get_rounded_scores(sentence, model, n_decimal=3)\n",
    "    label = int(np.argmax(scores))  # Extract label with the highest probability and cast to int\n",
    "    return scores, label\n",
    "\n",
    "\n",
    "\n",
    "def generate_best_paraphrase(input_sentence: str,\n",
    "                             min_levenshtein_distance: int = 30,\n",
    "                             min_length: int = 30,\n",
    "                             max_length: int = 60,\n",
    "                             num_return_sequences: int = 20,\n",
    "                             temperature: float = 1.5,\n",
    "                             top_k: int = 50,\n",
    "                             top_p: float = 0.95,\n",
    "                             cosine_similarity_threshold: float = 0.8,\n",
    "                             threshold_attempts = 300) -> str:\n",
    "    \"\"\"\n",
    "    Generates diverse paraphrases for a given input sentence and returns the best one\n",
    "    based on cosine similarity and constraints. Keeps generating paraphrases\n",
    "    until one has a cosine similarity score greater than the threshold.\n",
    "\n",
    "    Args:\n",
    "        input_sentence (str): Sentence to paraphrase.\n",
    "        min_levenshtein_distance (int): Minimum Levenshtein distance from input_sentence.\n",
    "        min_length (int): Minimum length constraint for generated paraphrases.\n",
    "        max_length (int): Maximum length constraint for generated paraphrases.\n",
    "        num_return_sequences (int): Number of paraphrases to generate per attempt.\n",
    "        temperature (float): Temperature parameter for diversity during sampling.\n",
    "        top_k (int): Top-k sampling parameter.\n",
    "        top_p (float): Nucleus sampling (top-p) parameter.\n",
    "        cosine_similarity_threshold (float): Minimum cosine similarity score for valid paraphrase.\n",
    "\n",
    "    Returns:\n",
    "        str: Best paraphrase based on cosine similarity or message indicating failure.\n",
    "    \"\"\"\n",
    "    # Load BART paraphrase model and tokenizer\n",
    "    bart_model = BartForConditionalGeneration.from_pretrained('eugenesiow/bart-paraphrase')\n",
    "    bart_tokenizer = BartTokenizer.from_pretrained('eugenesiow/bart-paraphrase')\n",
    "    bart_model = bart_model.to(device)\n",
    "\n",
    "    # Load DistilBERT model for embeddings\n",
    "    embedding_model = DistilBertModel.from_pretrained('lxyuan/distilbert-base-multilingual-cased-sentiments-student')\n",
    "    embedding_tokenizer = DistilBertTokenizer.from_pretrained('lxyuan/distilbert-base-multilingual-cased-sentiments-student')\n",
    "    embedding_model = embedding_model.to(device)\n",
    "\n",
    "    # Tokenize the input sentence\n",
    "    batch = bart_tokenizer(input_sentence, return_tensors='pt').to(device)\n",
    "\n",
    "\n",
    "    attempts = 0\n",
    "    # Loop to keep generating paraphrases until similarity score > 0.8\n",
    "    while attempts < threshold_attempts:\n",
    "        # Generate diverse paraphrases\n",
    "        generated_ids = bart_model.generate(\n",
    "            batch['input_ids'],\n",
    "            num_return_sequences=num_return_sequences,\n",
    "            num_beams=1,\n",
    "            temperature=temperature,\n",
    "            top_k=top_k,\n",
    "            top_p=top_p,\n",
    "            do_sample=True,\n",
    "            repetition_penalty=1.5,\n",
    "        )\n",
    "        paraphrases = bart_tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "\n",
    "        # Filter based on Levenshtein distance and length\n",
    "        valid_paraphrases = [\n",
    "            p for p in paraphrases if\n",
    "            Levenshtein.distance(input_sentence, p) >= min_levenshtein_distance and\n",
    "            min_length <= len(p) <= max_length\n",
    "        ]\n",
    "\n",
    "        # If valid paraphrases are found, proceed to similarity check\n",
    "        if valid_paraphrases:\n",
    "            # Function to compute embeddings\n",
    "            def compute_embedding(sentence):\n",
    "                inputs = embedding_tokenizer(sentence, return_tensors='pt', truncation=True, padding=True).to(device)\n",
    "                with torch.no_grad():\n",
    "                    outputs = embedding_model(**inputs)\n",
    "                return outputs.last_hidden_state.mean(dim=1).squeeze().cpu().numpy()\n",
    "\n",
    "            # Compute embeddings for the original sentence and valid paraphrases\n",
    "            original_embedding = compute_embedding(input_sentence)\n",
    "            paraphrase_embeddings = [compute_embedding(p) for p in valid_paraphrases]\n",
    "\n",
    "            # Compute cosine similarity\n",
    "            similarities = [cosine_similarity([original_embedding], [embedding])[0][0] for embedding in paraphrase_embeddings]\n",
    "\n",
    "            # Check if any paraphrase meets the similarity threshold\n",
    "            for i, similarity in enumerate(similarities):\n",
    "                if similarity > cosine_similarity_threshold:\n",
    "                    best_paraphrase = valid_paraphrases[i]\n",
    "                    return best_paraphrase\n",
    "\n",
    "        # If no valid paraphrase is found, continue the loop until success\n",
    "\n",
    "    return \"No valid paraphrase found after several attempts.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertForMaskedLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: CUDA-capable device(s) is/are busy or unavailable\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/hamdi/Documents/mlr-technical-test/demo/demo.ipynb Cell 3\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/hamdi/Documents/mlr-technical-test/demo/demo.ipynb#W2sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Define text transformations\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/hamdi/Documents/mlr-technical-test/demo/demo.ipynb#W2sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m transformation \u001b[39m=\u001b[39m CompositeTransformation([\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/hamdi/Documents/mlr-technical-test/demo/demo.ipynb#W2sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     WordSwapEmbedding(max_candidates\u001b[39m=\u001b[39m\u001b[39m30\u001b[39m),\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/hamdi/Documents/mlr-technical-test/demo/demo.ipynb#W2sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     WordSwapMaskedLM(method\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mbae\u001b[39;49m\u001b[39m\"\u001b[39;49m, max_candidates\u001b[39m=\u001b[39;49m\u001b[39m40\u001b[39;49m),\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/hamdi/Documents/mlr-technical-test/demo/demo.ipynb#W2sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     WordSwapRandomCharacterDeletion()\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/hamdi/Documents/mlr-technical-test/demo/demo.ipynb#W2sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m ])\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/hamdi/Documents/mlr-technical-test/demo/demo.ipynb#W2sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m# Define constraints\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/hamdi/Documents/mlr-technical-test/demo/demo.ipynb#W2sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m constraints \u001b[39m=\u001b[39m [\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/hamdi/Documents/mlr-technical-test/demo/demo.ipynb#W2sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     RepeatModification(),\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/hamdi/Documents/mlr-technical-test/demo/demo.ipynb#W2sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     StopwordModification(),\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/hamdi/Documents/mlr-technical-test/demo/demo.ipynb#W2sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     LevenshteinConstraint(target_sentence, min_levenshtein, min_length, max_length),\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/hamdi/Documents/mlr-technical-test/demo/demo.ipynb#W2sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m ]\n",
      "File \u001b[0;32m~/anaconda3/envs/Giskard/lib/python3.11/site-packages/textattack/transformations/word_swaps/word_swap_masked_lm.py:85\u001b[0m, in \u001b[0;36mWordSwapMaskedLM.__init__\u001b[0;34m(self, method, masked_language_model, tokenizer, max_length, window_size, max_candidates, min_confidence, batch_size, **kwargs)\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m     82\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m`tokenizer` argument must be provided when passing an actual model as `masked_language_model`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     83\u001b[0m         )\n\u001b[1;32m     84\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lm_tokenizer \u001b[39m=\u001b[39m tokenizer\n\u001b[0;32m---> 85\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_language_model\u001b[39m.\u001b[39;49mto(utils\u001b[39m.\u001b[39;49mdevice)\n\u001b[1;32m     86\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_language_model\u001b[39m.\u001b[39meval()\n\u001b[1;32m     87\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmasked_lm_name \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_language_model\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/Giskard/lib/python3.11/site-packages/transformers/modeling_utils.py:3110\u001b[0m, in \u001b[0;36mPreTrainedModel.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3105\u001b[0m     \u001b[39mif\u001b[39;00m dtype_present_in_args:\n\u001b[1;32m   3106\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   3107\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mYou cannot cast a GPTQ model in a new `dtype`. Make sure to load the model using `from_pretrained` using the desired\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   3108\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m `dtype` by passing the correct `torch_dtype` argument.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   3109\u001b[0m         )\n\u001b[0;32m-> 3110\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mto(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/Giskard/lib/python3.11/site-packages/torch/nn/modules/module.py:1340\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1337\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1338\u001b[0m             \u001b[39mraise\u001b[39;00m\n\u001b[0;32m-> 1340\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_apply(convert)\n",
      "File \u001b[0;32m~/anaconda3/envs/Giskard/lib/python3.11/site-packages/torch/nn/modules/module.py:900\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    898\u001b[0m \u001b[39mif\u001b[39;00m recurse:\n\u001b[1;32m    899\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 900\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    902\u001b[0m \u001b[39mdef\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    903\u001b[0m     \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    904\u001b[0m         \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    905\u001b[0m         \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    910\u001b[0m         \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    911\u001b[0m         \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/Giskard/lib/python3.11/site-packages/torch/nn/modules/module.py:900\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    898\u001b[0m \u001b[39mif\u001b[39;00m recurse:\n\u001b[1;32m    899\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 900\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    902\u001b[0m \u001b[39mdef\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    903\u001b[0m     \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    904\u001b[0m         \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    905\u001b[0m         \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    910\u001b[0m         \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    911\u001b[0m         \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/Giskard/lib/python3.11/site-packages/torch/nn/modules/module.py:900\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    898\u001b[0m \u001b[39mif\u001b[39;00m recurse:\n\u001b[1;32m    899\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 900\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    902\u001b[0m \u001b[39mdef\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    903\u001b[0m     \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    904\u001b[0m         \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    905\u001b[0m         \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    910\u001b[0m         \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    911\u001b[0m         \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/Giskard/lib/python3.11/site-packages/torch/nn/modules/module.py:927\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    923\u001b[0m \u001b[39m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    924\u001b[0m \u001b[39m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    925\u001b[0m \u001b[39m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    926\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> 927\u001b[0m     param_applied \u001b[39m=\u001b[39m fn(param)\n\u001b[1;32m    928\u001b[0m p_should_use_set_data \u001b[39m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    930\u001b[0m \u001b[39m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/Giskard/lib/python3.11/site-packages/torch/nn/modules/module.py:1326\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[39mif\u001b[39;00m convert_to_format \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m t\u001b[39m.\u001b[39mdim() \u001b[39min\u001b[39;00m (\u001b[39m4\u001b[39m, \u001b[39m5\u001b[39m):\n\u001b[1;32m   1320\u001b[0m         \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(\n\u001b[1;32m   1321\u001b[0m             device,\n\u001b[1;32m   1322\u001b[0m             dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1323\u001b[0m             non_blocking,\n\u001b[1;32m   1324\u001b[0m             memory_format\u001b[39m=\u001b[39mconvert_to_format,\n\u001b[1;32m   1325\u001b[0m         )\n\u001b[0;32m-> 1326\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39;49mto(\n\u001b[1;32m   1327\u001b[0m         device,\n\u001b[1;32m   1328\u001b[0m         dtype \u001b[39mif\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_floating_point() \u001b[39mor\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_complex() \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m   1329\u001b[0m         non_blocking,\n\u001b[1;32m   1330\u001b[0m     )\n\u001b[1;32m   1331\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m   1332\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mstr\u001b[39m(e) \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mCannot copy out of meta tensor; no data!\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: CUDA-capable device(s) is/are busy or unavailable\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define text transformations\n",
    "transformation = CompositeTransformation([\n",
    "    WordSwapEmbedding(max_candidates=30),\n",
    "    WordSwapMaskedLM(method=\"bae\", max_candidates=40),\n",
    "    WordSwapRandomCharacterDeletion()\n",
    "])\n",
    "\n",
    "# Define constraints\n",
    "constraints = [\n",
    "    RepeatModification(),\n",
    "    StopwordModification(),\n",
    "    LevenshteinConstraint(target_sentence, min_levenshtein, min_length, max_length),\n",
    "]\n",
    "\n",
    "# Define search method\n",
    "search_method = ParticleSwarmOptimization(pop_size=80, max_iters=40, post_turn_check=True, max_turn_retries=10)\n",
    "\n",
    "\n",
    "# Compute target scores and label dynamically\n",
    "target_scores, label = compute_target_scores_and_label(target_sentence)\n",
    "\n",
    "\n",
    "\n",
    "# Instantiate goal function\n",
    "goal_function = RoundedScoreGoal_prob(\n",
    "    model=wrapped_model,\n",
    "    target_scores=target_scores,\n",
    "    label_order=('positive', 'neutral', 'negative'),\n",
    "    n_decimal=3\n",
    ")\n",
    "\n",
    "\n",
    "# Define and run attack\n",
    "attack = textattack.Attack(goal_function, constraints, transformation, search_method)\n",
    "\n",
    "import time\n",
    "# Generate paraphrase dynamically\n",
    "generated_sentence = generate_best_paraphrase(target_sentence)\n",
    "start_time = time.time()\n",
    "attack_result = attack.attack(generated_sentence, label)\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f\"Attack took {elapsed_time:.2f} seconds.\")\n",
    "print(f\"The new text satisfying the criteria is: {attack_result}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Giskard",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
